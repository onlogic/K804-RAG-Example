{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302f1cf7-ce2c-4383-af28-70d592763f35",
   "metadata": {},
   "source": [
    "# Local LLM Example w/ RAG on OnLogic K800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a4431-7437-479c-8c96-82f0e3a6e083",
   "metadata": {},
   "source": [
    "This notebook shows an example pipeline for locally running an LLM with RAG integration, using Ollama and LangChain. \n",
    "\n",
    "Ollama is a user friendlly, free, and open-source tool for local LLM execution. For this example, we pull 2 models from Ollama:\n",
    "* Llama3.1:8b-instruct-fp16 - This is the general purpose model that the pipeline is built from. It works well on its own, but implementing a RAG allows for more control over responses and ensures the model adheres to reliable data sources. \n",
    "* nomic-embed-text - This is an Ollama supported text embedding model (137M parameters) that allows the pipeline to pull relevant context sources from the context database based on their similarity with the prompt.\n",
    "\n",
    "To get started with Ollama, visit their website and download the app for your device .\n",
    "After installing, run:\n",
    "\n",
    "`ollama pull llama3.1:8b-instruct-fp16`\n",
    "\n",
    "`ollama pull nomic-embed-text`\n",
    "\n",
    "to download and prepare the models for local execution. Ollama has a wide catalog of llms of different sizes and specialties, so browse to find one that matches your usecase and hardware.  \n",
    "\n",
    "Getting the models running is as easy as running\n",
    "\n",
    "`ollama start llama3.1:8b`\n",
    "\n",
    "Which allows you to chat with the LLM through the command line. Run `ollama ps` to see the models you have currently loaded in memory and `ollama list` to see your list of locally downloaded models. \n",
    "\n",
    "Langchain is the second tool used for this example, which is used to define the overall RAG pipeline. We first create an embeddings database using LangChain Chroma to store the embeddings for each context document, and then define a chain which collects the most relevent documents and provides them to the llama model which in turn responds to the question. This is a super simple example of the types of chains that LangChain supports, which can be much more complex and customizable, allowing for things like web search and model self-correction. \n",
    "\n",
    "\n",
    "All of the LangChain libraries used here can be installed by: \n",
    "\n",
    "`pip install langchain langchain_community`\n",
    "\n",
    "`pip install -qU langchain_chroma`\n",
    "\n",
    "`pip install -qU langchain_ollama`\n",
    "\n",
    "`pip install -qU langchain_unstructured`\n",
    "\n",
    "This notebook was run locally on an OnLogic K804 with one 20GB Nvidia A4000 GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbec6a-9394-4aa6-afdd-9fdb28a2e427",
   "metadata": {},
   "source": [
    "This example script will walk through the creation of a simple chat bot for giving technical support for a couple of onlogic pcs. To do this, we need to provide it context from which it will create its answers. To do this, we'll pull a collection of product manuals from onlogic.com, and let langchain and ollama convert them into embeddings, which allows the RAG to gain some semantic meaning of the content in each document. To do this, we'll use langchain's unstructured library to parse the documents and separate them into sections. This requires some additional installation, starting with \n",
    "\n",
    "`pip install -qU \"unstructured[pdf]\"`\n",
    "\n",
    "When parsing pdfs locally, the unstructured library also requires some additional dependencies, Tessaract and Poppler. \n",
    "\n",
    "You can also do pdf parsing through the free unstructured API, which requires an API key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cbdbeb2-d7b7-466d-9fd0-aa64df9de8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\no\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'product_manuals\\\\OnLogic-FR201-Product-Manual.pdf', 'file_directory': 'product_manuals', 'filename': 'OnLogic-FR201-Product-Manual.pdf', 'last_modified': '2024-12-04T12:09:29', 'page_number': 1, 'orig_elements': 'eJzlmE1v3DYQhv8KoXOo8vsjt6aN2xRNY8TOyQ4MihzZAnalhVbrjwb97x1RMrCxt8D2oEO3x5f7ckXx0QxnePWtgBWsoR1umlS8JYWK0kVhHa0rnagyUtMgDKdcV44pGStWV8UbUqxhCCkMAed8K2LX9alpwwDbrFfhqdsNN3fQ3N4NOOKUwCnz6EOThjsc1N7g4KZr2mGcdXVlRcneEK5t6b++Ic/SyUlKoUp7QGc7DhTbp+0A6/EdzptHWF1sQoTiL/yhblZwk5oe4tD1T6Nh03dpF4ebdWh3YbUtZlMb1jD+/Kn9vbttIj37LBin55OZfszmcpPqIr9Me7sLt/mFrwpob4uveXSL/9qlpm4gb6dgQlEuKFOXXLxl/q3w4+wNzrxpd+sKenTxcZUDPI5bVeSHkvmhZHroOGV42uTFXTbDKr/XK3LGuJpxRoPmjiqoAnXgNHV1srwONnkvFyVnbWn2wDE5SaFNaV7r7P5vcxP73D7DfbNtupb82mzzeo9hBsyC9zFRZyOnigVGXbCBKmGDcwZYpRaLNluijUtXugxtkmqWXPmSH9DZ/n+nZqRT4B2jqRKGqqQDDSAcFVxhqkwRKi2WpWZEyfeo2VkKzg7qbD8hamdNvx1Ij1TCFkhXkylpro9PlrXTpgYWqYAx8IKReMwlTrULXEeNgSgXS5YTM6dLuYfQz1IwVsoDOttPCOGXDW4rkHddN5DfdusN9OS8aXHjjsIXRAjAlKJJ+EhVHST1xlW0jjpxW9UWa5hF8Qnm5sNuknyWQugxT77S2X5C+EQpycduuIO+6kKfyE9d2+LKyYT1KIZW1aCESFQKr7FekYY6lxjVYGP0lgfD/LIMJSvZHkMtJim9GUPulc72E2L4EWdf7+qKcUgEUZLr4qzv2oF8+OHTdUEemuGOXO9wYZE/ki8X74gsBZZxv1Q/bPNwIh9a3IiQxgw8GcVBo9//Hr60Eenddn3zJ6TLcSkHvg0XUjQRNNU+VVTJhLVsrRKFFC02ItJ6z5b9NozdP2GFnaWwOn8LL3W2n9C3McUxfhYdadq42iUgLTyQZtut8vh5957gE3ar72L9j9D3YWju4Z+4mqSwPIJIjdcKuVaSOq0stcxZIT0IRL1ojyK52e8upRKTHAHaAzrbT4jrZahWuWLChD3AuDdHHbfGSemYoUwrDEcBeNx6bjA6MTxFiNZKvSw2x0u1h82LSXIrM6aXOttPCBsnlFzkVyGf7qG/b+DhKHDRVaIWBvOoSR7BeU8r5WtahSoYH6MQii8EzrORlGJzvM2Sz5c7AsfVAc1OLN54yZ/BfWiH6dnYbh4HTzoZAByeegF7lDFtOhCIEQ/AoOrkKqOWhSf5VAHNUs0F0QjLHNDZflLwBEVstOoeyY8xwnaLVQsclzKZSCbVmlMnoKaKuYgdiklUWyd50IpJtlQFM9N6vkedpXFzpAk1diSv9Anco76AJzFpPt+jXmwgzsUulp8YgUdStJELhVnT1AGosgmot0JQDzzFVFWai6XqlRmbE9+FoJdzyDmXqb3U2X5SFBVSfP84QN90PTmDMOx62JLQJvJzg6S2R7OMUspogFFsxrH2jMrQwPFo5LVMrMJ+I5il7gw4tv9IT7M55p4197O2OYG+0tl/WjTLsZS57DZjk/mv20IZbEhWYjOYtMZwlIn6hOFoOKurwJ3zsNSt3TMyOQfcs1Zq1t6Nlecrnf0nhlAgwnfdMHTrlxTn4Pv6N8c2cKs=', 'filetype': 'application/pdf', 'category': 'CompositeElement', 'element_id': '6284e3e471bead1d5377d56608d34505'}, page_content='FR201 Product Manual\\n\\nRevision History\\n\\nRevision History\\n\\nFirst release of FR201 manual\\n\\nUpdate Boot Jumper Pinout\\n\\n2.3 Motherboard Connect Update\\n\\nModiﬁed the \"Front I/O\" with “1x USB 3.2 10Gb/s” Instead of “2x USB 3.2 10Gb/s’\\n\\nUpdated to include new isolated PoE module\\n\\nTable of Contents\\n\\n1 - System Overview\\n\\n1.1 System Introduction\\n\\n1.2- In-box Accessories\\n\\n1.3 - Product Speciﬁcations\\n\\n1.4 - Exterior Features and Dimensions\\n\\n1.4.1 - Top I/O\\n\\n1.4.2 - Bottom I/O')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse pdfs, creating a Document object for each section within each pdf\n",
    "# this can take a while, depending on the number and size of the documents. \n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from glob import glob\n",
    "\n",
    "docs = []\n",
    "\n",
    "pdf_filepaths = glob('product_manuals/*.pdf')\n",
    "\n",
    "for file in pdf_filepaths:\n",
    "    loader = UnstructuredLoader(\n",
    "        file_path=file,\n",
    "        strategy=\"fast\",\n",
    "        chunking_strategy='by_title'\n",
    "    )\n",
    "\n",
    "    for doc in loader.lazy_load():\n",
    "        docs.append(doc)\n",
    "    \n",
    "filter_complex_metadata(docs)\n",
    "\n",
    "print(f'Total number of documents: {len(docs)}')\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0cb4b8-c9a1-48b6-b0b7-0a4662c009bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Use a local embedding model to store the embddings of all context documents in a vector database\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, local_embeddings)\n",
    "\n",
    "# formatting function used by the Llama model to \"read\" the context documents.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f'context source: {doc.metadata[\"filename\"]}\\nContent:\\n{doc.page_content}' for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f46cfd-bf38-47f4-87e9-65b4e9b9d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.9 - DisplayPort\\n\\nThere are two full-size DisplayPorts on the Karbon 800 series. Both ports support DP 1.4 at 4K 60Hz\\n\\nand support MST (Multi Stream Topology). An MST hub can be used to support up to four\\n\\nindependent displays. Please refer to Intel documentation for additional Alder Lake-S display output\\n\\nspecifications: https://ark.intel.com/\\n\\n22'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the vector store\n",
    "query = \"What are the displayport details for the k800 series?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(query)\n",
    "# Retrieves the most semantically similar context document to the above query: \n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6450c7-4ea5-4f13-aa12-bea12ed9e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load Llama3.1 model\n",
    "\n",
    "model = ChatOllama(\n",
    "    model='llama3.1:8b'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8b2462-a097-45f4-8c3d-b4d00611b59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM with context retrieval\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Define the chain\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for tech support question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# The chain finds the most relevent context document using the vectorstore retriever, formats it,\n",
    "# and inputs it into the template along with the user quesion. Then the model executues on the entire prompt, \n",
    "# and returns a response object. \n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0158e1-2117-4f45-a64e-d45def5d1445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the details for the K800 series of computers' CE compliance?\"\n",
    "response = chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa5d8044-33f7-47dc-828f-07f245443156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The K800 series was evaluated for medical, IT equipment, automotive, maritime, and railway EMC standards as a class A device. It complies with the relevant IT equipment directives for the CE mark. Testing includes EN 55032, EN 55035, and other specified standards.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5584460c-16c5-486b-b303-f974f059127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total response duration: 2.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# Total run duration in seconds\n",
    "total_dur = response.response_metadata['total_duration'] /  1e9\n",
    "print(f'Total response duration: {total_dur:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bac8cb14-a7bf-4fbb-b8ec-dc8525fb2acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to first token: 0.27 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time to first token\n",
    "print(f\"Time to first token: {response.response_metadata['prompt_eval_duration'] / 1e9:,.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2587dcb-9a4d-47ab-8d73-78c4aa253cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output words per minute: 3,183 WPM\n"
     ]
    }
   ],
   "source": [
    "output_tokens_per_s = response.usage_metadata['output_tokens'] / response.response_metadata['eval_duration'] * 1e9\n",
    "output_words_per_s = output_tokens_per_s / 1.3\n",
    "output_words_per_m = output_words_per_s * 60\n",
    "\n",
    "print(f'LLM output words per minute: {output_words_per_m:,.0f} WPM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd474c27-e8cd-4f08-93b2-735f8f30707e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
